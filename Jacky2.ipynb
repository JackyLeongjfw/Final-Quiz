{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f05e155",
   "metadata": {},
   "source": [
    "## Lasso SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Lasso_SVD(BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_users, n_items, lam=0.1, K=5, iterNum=10, tol=1e-4, verbose=1):\n",
    "        self.P = np.random.randn(n_users, K)        # user latent factors\n",
    "        self.Q = np.random.randn(n_items, K)        # item latent factors\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.K = K                                  # latent dimension\n",
    "        self.lam = lam                              # L1 regularization strength\n",
    "        self.iterNum = iterNum                      # number of ALS iterations\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Precompute indices for each user and item\n",
    "        self.index_item = [np.where(X[:,1] == i)[0] for i in range(self.n_items)]\n",
    "        self.index_user = [np.where(X[:,0] == u)[0] for u in range(self.n_users)]\n",
    "\n",
    "        for _ in range(self.iterNum):\n",
    "\n",
    "            # -------- update item latent factors Q --------\n",
    "            for item_id in range(self.n_items):\n",
    "                idx = self.index_item[item_id]\n",
    "\n",
    "                if len(idx) == 0:\n",
    "                    self.Q[item_id] = 0\n",
    "                    continue\n",
    "\n",
    "                users = X[idx][:, 0]\n",
    "                P_tmp = self.P[users]\n",
    "                y_tmp = y[idx]\n",
    "\n",
    "                # Lasso regression update\n",
    "                clf = Lasso(alpha=self.lam, fit_intercept=False)\n",
    "                clf.fit(P_tmp, y_tmp)\n",
    "                self.Q[item_id, :] = clf.coef_\n",
    "\n",
    "            # -------- update user latent factors P --------\n",
    "            for user_id in range(self.n_users):\n",
    "                idx = self.index_user[user_id]\n",
    "\n",
    "                if len(idx) == 0:\n",
    "                    self.P[user_id] = 0\n",
    "                    continue\n",
    "\n",
    "                items = X[idx][:, 1]\n",
    "                Q_tmp = self.Q[items]\n",
    "                y_tmp = y[idx]\n",
    "\n",
    "                clf = Lasso(alpha=self.lam, fit_intercept=False)\n",
    "                clf.fit(Q_tmp, y_tmp)\n",
    "                self.P[user_id, :] = clf.coef_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # dot product pᵤ^T qᵢ\n",
    "        return np.array([np.dot(self.P[u], self.Q[i]) for u, i in X])\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_pred, y_true):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv\")\n",
    "test  = pd.read_csv(\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = train[['User', 'Item']].values\n",
    "y_train = train['Rating'].values\n",
    "\n",
    "X_test = test[['User', 'Item']].values\n",
    "y_test = test['Rating'].values\n",
    "\n",
    "# Get user/item counts\n",
    "n_users = max(train['User'].max(), test['User'].max()) + 1\n",
    "n_items = max(train['Item'].max(), test['Item'].max()) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c77487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso_SVD(n_users, n_items, lam=0.1, K=3, iterNum=8)\n",
    "model.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "rmse_1 = root_mean_squared_error(pred_test, y_test)\n",
    "print(\"RMSE (lambda=0.1, K=3):\", rmse_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso_SVD(n_users, n_items, lam=0.3, K=5, iterNum=8)\n",
    "model.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "rmse_2 = root_mean_squared_error(pred_test, y_test)\n",
    "print(\"RMSE (lambda=0.3, K=5):\", rmse_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754270f",
   "metadata": {},
   "source": [
    "### **Q1.1**\n",
    "- Standardize features by removing the mean and scaling to unit variance.\n",
    "- Merge `data.data` and `data.target` as a one `dataframe` with columns: [`sepal length (cm)`,\t`sepal width (cm)`, `petal length (cm)`, `petal width (cm)`, `target`]\n",
    "- Compute the target-specific mean of each features, that is,\n",
    "\n",
    "              sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
    "      target\n",
    "      0                   5.006             3.428              1.462             0.246\n",
    "      1                   5.936             2.770              4.260             1.326\n",
    "      2                   6.588             2.974              5.552             2.026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36468948",
   "metadata": {},
   "source": [
    "### **Q1.2**\n",
    "\n",
    "- Using `seaborn` to show the `Violinplot` of all features against `target`\n",
    "- Using `seaborn` to show the `heatmap` of the correlation between all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = load_iris(as_frame=True)\n",
    "\n",
    "# Extract X and y\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# ---- 1. Standardize features ----\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_std_df = pd.DataFrame(X_std, columns=X.columns)\n",
    "\n",
    "# ---- 2. Merge standardized features + target ----\n",
    "df = X_std_df.copy()\n",
    "df['target'] = y\n",
    "\n",
    "# Show merged DataFrame\n",
    "df.head()\n",
    "\n",
    "# ---- 3. Compute target-specific mean ----\n",
    "group_mean = df.groupby('target').mean()\n",
    "group_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47d654",
   "metadata": {},
   "source": [
    "## **Q3: Implementing a Custom Recommender System using TensorFlow**\n",
    "\n",
    "**Recommender System Model:**\n",
    "\n",
    "The goal of this question is to implement a custom recommender system model using TensorFlow. The model is defined as follows:\n",
    "\n",
    "$$\\widehat{r}_{ui} = \\mathbf{p}_u^\\intercal \\mathbf{q}_i + b_i + \\mathbf{p}_u^\\intercal \\mathbf{p}_u + a_u$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\widehat{r}_{ui}$ is the predicted rating for user $u$ and item $i$\n",
    "* $\\mathbf{p}_u$ is the user latent factor vector\n",
    "* $\\mathbf{q}_i$ is the item latent factor vector\n",
    "* $b_i$ is the item bias term\n",
    "* $a_u$ is the user bias term\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "Implement this model using TensorFlow on our course dataset. You will need to:\n",
    "\n",
    "1. Load the dataset and preprocess the data as needed\n",
    "2. Define the model architecture using TensorFlow\n",
    "3. Implement the loss function and optimizer\n",
    "4. Train the model on the dataset\n",
    "5. Evaluate the performance of the model using a `Acc`.\n",
    "\n",
    "**Note:** You can use TensorFlow's built-in functions and modules to implement the model. Note that the prediction result is not of importance; this question only assesses your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f746adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow recommender implementing r_hat = p_u^T q_i + b_i + p_u^T p_u + a_u\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "\n",
    "# ---------------------------\n",
    "# Helper metrics\n",
    "# ---------------------------\n",
    "def rmse_tf(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "def rounded_accuracy(y_true, y_pred):\n",
    "    # simple \"Acc\" that checks if rounded(pred) == rounded(true)\n",
    "    y_true_r = tf.round(y_true)\n",
    "    y_pred_r = tf.round(y_pred)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true_r, y_pred_r), tf.float32))\n",
    "\n",
    "# ---------------------------\n",
    "# Model definition\n",
    "# ---------------------------\n",
    "class CustomRecModel(tf.keras.Model):\n",
    "    def __init__(self, n_users, n_items, K=8, reg=0.0):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        # Embeddings: shape (n_users, K) and (n_items, K)\n",
    "        self.user_f = layers.Embedding(\n",
    "            input_dim=n_users, output_dim=K,\n",
    "            embeddings_initializer=\"random_normal\",\n",
    "            embeddings_regularizer=regularizers.l2(reg),\n",
    "            name=\"user_f\"\n",
    "        )\n",
    "        self.item_f = layers.Embedding(\n",
    "            input_dim=n_items, output_dim=K,\n",
    "            embeddings_initializer=\"random_normal\",\n",
    "            embeddings_regularizer=regularizers.l2(reg),\n",
    "            name=\"item_f\"\n",
    "        )\n",
    "        # Bias embeddings (scalars)\n",
    "        self.user_bias = layers.Embedding(input_dim=n_users, output_dim=1,\n",
    "                                          embeddings_initializer=\"zeros\",\n",
    "                                          name=\"user_bias\")\n",
    "        self.item_bias = layers.Embedding(input_dim=n_items, output_dim=1,\n",
    "                                          embeddings_initializer=\"zeros\",\n",
    "                                          name=\"item_bias\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs: (users, items) as integer tensors\n",
    "        users, items = inputs\n",
    "        p_u = self.user_f(users)                # shape (batch, K)\n",
    "        q_i = self.item_f(items)                # shape (batch, K)\n",
    "        a_u = tf.squeeze(self.user_bias(users), axis=-1)  # shape (batch,)\n",
    "        b_i = tf.squeeze(self.item_bias(items), axis=-1)  # shape (batch,)\n",
    "\n",
    "        # dot product p_u^T q_i\n",
    "        dot = tf.reduce_sum(p_u * q_i, axis=1)          # shape (batch,)\n",
    "\n",
    "        # p_u^T p_u  (vector self-interaction)\n",
    "        pu_norm = tf.reduce_sum(p_u * p_u, axis=1)      # shape (batch,)\n",
    "\n",
    "        # final prediction\n",
    "        r_hat = dot + b_i + pu_norm + a_u               # shape (batch,)\n",
    "        return r_hat\n",
    "\n",
    "# ---------------------------\n",
    "# Data loading example (Netflix-like CSVs)\n",
    "# Replace these with your course dataset paths as needed\n",
    "# ---------------------------\n",
    "train_url = \"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv\"\n",
    "test_url  = \"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_url)\n",
    "test  = pd.read_csv(test_url)\n",
    "\n",
    "# ensure zero-based indices for users/items (if not already)\n",
    "# If your CSV already uses zero-based ids, keep as-is.\n",
    "# If ids start at 1, subtract 1:\n",
    "if train['User'].min() == 1:\n",
    "    train['User'] -= 1\n",
    "    test['User']  -= 1\n",
    "if train['Item'].min() == 1:\n",
    "    train['Item'] -= 1\n",
    "    test['Item']  -= 1\n",
    "\n",
    "# numpy arrays for training\n",
    "X_train_users = train['User'].to_numpy().astype(np.int32)\n",
    "X_train_items = train['Item'].to_numpy().astype(np.int32)\n",
    "y_train = train['Rating'].to_numpy().astype(np.float32)\n",
    "\n",
    "X_test_users = test['User'].to_numpy().astype(np.int32)\n",
    "X_test_items = test['Item'].to_numpy().astype(np.int32)\n",
    "y_test = test['Rating'].to_numpy().astype(np.float32)\n",
    "\n",
    "n_users = max(train['User'].max(), test['User'].max()) + 1\n",
    "n_items = max(train['Item'].max(), test['Item'].max()) + 1\n",
    "\n",
    "# ---------------------------\n",
    "# Build, compile and train model\n",
    "# ---------------------------\n",
    "K = 8\n",
    "reg = 1e-5\n",
    "model = CustomRecModel(n_users=n_users, n_items=n_items, K=K, reg=reg)\n",
    "\n",
    "# compile with MSE and custom metrics\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[rmse_tf, rounded_accuracy]\n",
    ")\n",
    "\n",
    "# Prepare tf.data datasets\n",
    "batch_size = 1024\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(((X_train_users, X_train_items), y_train))\n",
    "train_ds = train_ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(((X_test_users, X_test_items), y_test))\n",
    "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Fit\n",
    "history = model.fit(train_ds, epochs=10, validation_data=test_ds)\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate and print final metrics\n",
    "# ---------------------------\n",
    "eval_res = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"Test results:\", eval_res)\n",
    "\n",
    "# Example: predict first 10\n",
    "preds = model.predict((X_test_users[:10], X_test_items[:10])).flatten()\n",
    "print(\"First 10 predictions:\", preds)\n",
    "print(\"First 10 true ratings:\", y_test[:10])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
