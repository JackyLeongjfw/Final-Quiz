{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 1: Data Preprocessing & Evaluation\n"
      ],
      "metadata": {
        "id": "bFhowMNUkDj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Evaluation Metrics (MAPE) (Solution for Q1.1)\n",
        "Key Concept: RMSE is standard. MAPE is tricky because actual ratings can be 0 (division by zero error). We handle this with an epsilon or by masking."
      ],
      "metadata": {
        "id": "8dG7E-zokISW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def MAPE(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100.0\n",
        "    return mape\n",
        "actual_ratings = np.array([4.0, 3.0, 5.0, 2.0, 4.5])\n",
        "predicted_ratings = np.array([3.8, 3.2, 4.8, 2.2, 4.2])\n",
        "\n",
        "print(\"MAPE:\", MAPE(actual_ratings, predicted_ratings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCB9ElIBq1rn",
        "outputId": "ad44749a-4fa8-4c24-dd76-7ef8e8f21598"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE: 6.4666666666666694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Solution for Q1.2)"
      ],
      "metadata": {
        "id": "9CdPeF830lRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def MAPE_ignore_zero(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    mask = (y_true != 0)\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0\n",
        "\n",
        "actual_ratings = np.array([4.0, 3.0, 5.0, 2.0, 4.5, 0.0])\n",
        "predicted_ratings = np.array([3.8, 3.2, 4.8, 2.2, 4.2, 1.0])\n",
        "print(\"Tolerant MAPE (ignore zeros):\", MAPE_ignore_zero(actual_ratings, predicted_ratings))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9fqdZQTkNbo",
        "outputId": "340bd15b-f183-41e4-893b-276b69b1a066"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tolerant MAPE (ignore zeros): 6.4666666666666694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Data Loading & ID Encoding\n",
        "Key Concept: Recommender systems (SVD/Neural) require User and Item IDs to be continuous integers starting from 0. We use LabelEncoder for this."
      ],
      "metadata": {
        "id": "0ggGlo2pkTHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load Data (Example)\n",
        "# df = pd.read_csv('filepath.csv')\n",
        "# Assume df has columns: 'user_id', 'item_id', 'rating'\n",
        "\n",
        "# 2. Encode IDs (CRITICAL STEP)\n",
        "# We combine train and test sets to ensure the encoder knows ALL users/items\n",
        "user_le = LabelEncoder()\n",
        "all_users = df['user_id'].unique()\n",
        "user_le.fit(all_users)\n",
        "df['user_id'] = user_le.transform(df['user_id'])\n",
        "n_users = len(user_le.classes_)\n",
        "\n",
        "item_le = LabelEncoder()\n",
        "all_items = df['item_id'].unique()\n",
        "item_le.fit(all_items)\n",
        "df['item_id'] = item_le.transform(df['item_id'])\n",
        "n_items = len(item_le.classes_)\n",
        "\n",
        "# 3. Split Data\n",
        "X = df[['user_id', 'item_id']].values\n",
        "y = df['rating'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "d6eEi5mWkYAF",
        "outputId": "34628a65-268f-46b6-8b89-13c00aeb4945"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-373464619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# We combine train and test sets to ensure the encoder knows ALL users/items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0muser_le\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mall_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0muser_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 2: Baseline Methods"
      ],
      "metadata": {
        "id": "AFkfrrsCkZKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Median-Based Recommender (Solution for Q2.1)\n",
        "Key Concept: Instead of np.mean, we use np.median which is robust to outliers."
      ],
      "metadata": {
        "id": "Skn_45gFkfPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_url = 'https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv'\n",
        "test_url  = 'https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv'"
      ],
      "metadata": {
        "id": "CfyutjZnoyx0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ======================================================\n",
        "# Load dataset\n",
        "# ======================================================\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv')\n",
        "\n",
        "# ======================================================\n",
        "# 1. USER MEDIAN RECOMMENDER\n",
        "# ======================================================\n",
        "\n",
        "class UserMedianRS(BaseEstimator):\n",
        "\n",
        "    def fit(self, train_df):\n",
        "        self.user_median = train_df.groupby(\"user_id\")[\"rating\"].median().to_dict()\n",
        "        self.global_median = train_df[\"rating\"].median()\n",
        "        return self\n",
        "\n",
        "    def predict(self, df):\n",
        "        preds = []\n",
        "        for uid in df[\"user_id\"]:\n",
        "            if uid in self.user_median:\n",
        "                preds.append(self.user_median[uid])\n",
        "            else:\n",
        "                preds.append(self.global_median)\n",
        "        return np.array(preds)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 2. ITEM MEDIAN RECOMMENDER\n",
        "# ======================================================\n",
        "\n",
        "class ItemMedianRS(BaseEstimator):\n",
        "\n",
        "    def fit(self, train_df):\n",
        "        self.item_median = train_df.groupby(\"movie_id\")[\"rating\"].median().to_dict()\n",
        "        self.global_median = train_df[\"rating\"].median()\n",
        "        return self\n",
        "\n",
        "    def predict(self, df):\n",
        "        preds = []\n",
        "        for mid in df[\"movie_id\"]:\n",
        "            if mid in self.item_median:\n",
        "                preds.append(self.item_median[mid])\n",
        "            else:\n",
        "                preds.append(self.global_median)\n",
        "        return np.array(preds)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3. FITTING + PREDICTING + RMSE\n",
        "# ======================================================\n",
        "\n",
        "def rmse(true, pred):\n",
        "    return np.sqrt(mean_squared_error(true, pred))\n",
        "\n",
        "\n",
        "# ----- User Median -----\n",
        "user_model = UserMedianRS()\n",
        "user_model.fit(train)\n",
        "\n",
        "user_pred = user_model.predict(test)\n",
        "user_rmse = rmse(test[\"rating\"], user_pred)\n",
        "\n",
        "# ----- Item Median -----\n",
        "item_model = ItemMedianRS()\n",
        "item_model.fit(train)\n",
        "\n",
        "item_pred = item_model.predict(test)\n",
        "item_rmse = rmse(test[\"rating\"], item_pred)\n",
        "\n",
        "print(f\"User Median RS RMSE: {user_rmse:.4f}\")\n",
        "print(f\"Item Median RS RMSE: {item_rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIxjJ2h1kpPP",
        "outputId": "be03be04-da4c-4d46-eee8-13ce7f66a1a6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Median RS RMSE: 1.0840\n",
            "Item Median RS RMSE: 1.1090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Hybrid Baseline (Average of User & Item Models) (Solution for Q2.2)\n",
        "Key Concept: Combine predictions from two models to reduce variance."
      ],
      "metadata": {
        "id": "7UjoxsiUks1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ======================================================\n",
        "# Load dataset\n",
        "# ======================================================\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv')\n",
        "\n",
        "# ======================================================\n",
        "# 1. Average User–Item Median Recommender\n",
        "# ======================================================\n",
        "\n",
        "class AveUserItemMedianRS(BaseEstimator):\n",
        "\n",
        "    def fit(self, train_df):\n",
        "        self.user_median = train_df.groupby(\"user_id\")[\"rating\"].median().to_dict()\n",
        "        self.item_median = train_df.groupby(\"movie_id\")[\"rating\"].median().to_dict()\n",
        "        self.global_median = train_df[\"rating\"].median()\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict(self, df):\n",
        "        preds = []\n",
        "\n",
        "        for uid, mid in zip(df[\"user_id\"], df[\"movie_id\"]):\n",
        "            if uid in self.user_median:\n",
        "                u_med = self.user_median[uid]\n",
        "            else:\n",
        "                u_med = self.global_median\n",
        "\n",
        "            if mid in self.item_median:\n",
        "                i_med = self.item_median[mid]\n",
        "            else:\n",
        "                i_med = self.global_median\n",
        "\n",
        "            preds.append(0.5 * (u_med + i_med))\n",
        "\n",
        "        return np.array(preds)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 2. RMSE Helper\n",
        "# ======================================================\n",
        "\n",
        "def rmse(true, pred):\n",
        "    return np.sqrt(mean_squared_error(true, pred))\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 3. FIT, PREDICT, EVALUATE\n",
        "# ======================================================\n",
        "\n",
        "model = AveUserItemMedianRS()\n",
        "model.fit(train)\n",
        "\n",
        "pred = model.predict(test)\n",
        "score = rmse(test[\"rating\"], pred)\n",
        "\n",
        "print(f\"AveUserItemMedianRS RMSE: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzyWUOlkkuGd",
        "outputId": "10e971ce-7d01-477e-9b3a-4680301d57c3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveUserItemMedianRS RMSE: 1.0032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 3: Matrix Factorization (SVD)"
      ],
      "metadata": {
        "id": "RZmT8ISYkxdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Standard SVD (ALS with Ridge) (Solution for Q3.1)\n",
        "Key Concept: Alternating Least Squares (ALS). Fix User Matrix $P$, solve Item Matrix $Q$. Fix $Q$, solve $P$."
      ],
      "metadata": {
        "id": "g_5rhAiKkyhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O TabRS.py https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/src/TabRS.py\n",
        "from TabRS import SVD\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWGqeGCPv5ll",
        "outputId": "66261a75-0313-4733-8709-68831caddcd8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-26 06:00:21--  https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/src/TabRS.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9001 (8.8K) [text/plain]\n",
            "Saving to: ‘TabRS.py’\n",
            "\n",
            "TabRS.py            100%[===================>]   8.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-26 06:00:21 (18.8 MB/s) - ‘TabRS.py’ saved [9001/9001]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from TabRS import SVD\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv\")\n",
        "\n",
        "X_train = train[[\"user_id\", \"movie_id\"]].values\n",
        "y_train = train[\"rating\"].values\n",
        "\n",
        "X_test  = test[[\"user_id\", \"movie_id\"]].values\n",
        "y_test  = test[\"rating\"].values\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# Initialize SVD Model\n",
        "# ======================================================\n",
        "n_users = train[\"user_id\"].max() + 1\n",
        "n_items = train[\"movie_id\"].max() + 1\n",
        "\n",
        "model = SVD(\n",
        "    n_users=n_users,\n",
        "    n_items=n_items,\n",
        "    K=10,\n",
        "    lam=0.02,\n",
        "    iterNum=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# Fit SVD\n",
        "# ======================================================\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ======================================================\n",
        "# Predict on test set\n",
        "# ======================================================\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "# ======================================================\n",
        "# Compute RMSE\n",
        "# ======================================================\n",
        "score = rmse(y_test, pred)\n",
        "\n",
        "print(f\"Reg-SVD Test RMSE: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDnf4rBk28o",
        "outputId": "4e7ec9bd-8f88-4941-9dd2-b2754848a9d7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting Reg-SVD: K: 10, lam: 0.02000\n",
            "RegSVD-ALS: 0; obj: 1.307; rmse:1.138, diff: 1140.396\n",
            "RegSVD-ALS: 1; obj: 0.762; rmse:0.873, diff: 0.545\n",
            "RegSVD-ALS: 2; obj: 0.760; rmse:0.872, diff: 0.003\n",
            "RegSVD-ALS: 3; obj: 0.759; rmse:0.871, diff: 0.000\n",
            "RegSVD-ALS: 4; obj: 0.759; rmse:0.871, diff: 0.000\n",
            "Reg-SVD Test RMSE: 0.9711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Overfitting vs Underfitting (Solution for Q3.2)\n",
        "\n",
        "Concept: If Training RMSE drops significantly but Validation RMSE remains high, the model is overfitting.\n",
        "\n",
        "Solution: Increase Regularization ($\\lambda$)  / Decrease Complexity (Lower $K$) (Maybe but K = 3 is already small)."
      ],
      "metadata": {
        "id": "wdbYmlB6k5s8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Hyperparameter Tuning (Solution for Q3.3)"
      ],
      "metadata": {
        "id": "o3dr3ndllLaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Load SVD implementation\n",
        "# -------------------------------------------------------------------\n",
        "!wget -O TabRS.py https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/src/TabRS.py\n",
        "from TabRS import SVD\n",
        "\n",
        "\n",
        "# RMSE scorer for GridSearchCV\n",
        "def rmse_score(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Synthetic dataset\n",
        "# -------------------------------------------------------------------\n",
        "data = {\n",
        "    'user_id': [0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10],\n",
        "    'item_id': [0,2,1,2,1,3,1,3,2,3,2,3,4,5,4,5,6,7,6,7,8,9],\n",
        "    'rating':  [3,5,5,3,4,2,1,3,4,5,2,3,3,4,4,5,2,3,3,4,4,5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['user_id','item_id']].values\n",
        "y = df['rating'].values\n",
        "\n",
        "n_users = df.user_id.max()+1\n",
        "n_items = df.item_id.max()+1\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Wrapper class to make SVD compatible with GridSearchCV\n",
        "# -------------------------------------------------------------------\n",
        "class SVD_Wrapper(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, K=10, lam=0.01):\n",
        "        self.K = K\n",
        "        self.lam = lam\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = SVD(\n",
        "            n_users=n_users,\n",
        "            n_items=n_items,\n",
        "            K=self.K,\n",
        "            lam=self.lam,\n",
        "            iterNum=10,\n",
        "            verbose=0\n",
        "        )\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Hyperparameter Grid\n",
        "# -------------------------------------------------------------------\n",
        "param_grid = {\n",
        "    'K': [2, 5, 10],\n",
        "    'lam': [0.01, 0.03, 0.05]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    SVD_Wrapper(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_root_mean_squared_error'\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Fit GridSearchCV\n",
        "# -------------------------------------------------------------------\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best RMSE:\", -grid.best_score_)\n",
        "\n",
        "print(\"\\nCV Results:\")\n",
        "cv_results = pd.DataFrame(grid.cv_results_)\n",
        "print(cv_results[['param_K','param_lam','mean_test_score']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkc5ueyulIWV",
        "outputId": "56b99435-e6c1-42e1-a76d-e8c7526fd5d4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-26 06:05:48--  https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/src/TabRS.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9001 (8.8K) [text/plain]\n",
            "Saving to: ‘TabRS.py’\n",
            "\n",
            "\rTabRS.py              0%[                    ]       0  --.-KB/s               \rTabRS.py            100%[===================>]   8.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-26 06:05:48 (86.8 MB/s) - ‘TabRS.py’ saved [9001/9001]\n",
            "\n",
            "Best params: {'K': 2, 'lam': 0.05}\n",
            "Best RMSE: 1.1006769121106124\n",
            "\n",
            "CV Results:\n",
            "   param_K  param_lam  mean_test_score\n",
            "0        2       0.01        -1.352196\n",
            "1        2       0.03        -1.631848\n",
            "2        2       0.05        -1.100677\n",
            "3        5       0.01        -1.286755\n",
            "4        5       0.03        -1.438986\n",
            "5        5       0.05        -1.262419\n",
            "6       10       0.01        -1.426531\n",
            "7       10       0.03        -1.391376\n",
            "8       10       0.05        -1.189849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Huber SVD (Solution for Q3.4)\n",
        "Key Concept: Replace Ridge with HuberRegressor to handle outliers (noise) better."
      ],
      "metadata": {
        "id": "MZZW_esIlFwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import HuberRegressor, Ridge\n",
        "\n",
        "class Huber_SVD:\n",
        "\n",
        "    def __init__(self, n_users, n_items, K=3, lam=0.1, delta=1.35, n_iter=10):\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.K = K\n",
        "        self.lam = lam\n",
        "        self.delta = delta\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        # initialize latent factors\n",
        "        self.P = 0.1 * np.random.randn(self.n_users, self.K)\n",
        "        self.Q = 0.1 * np.random.randn(self.n_items, self.K)\n",
        "\n",
        "        # build user/item index lists\n",
        "        self.user_ratings = {}\n",
        "        self.item_ratings = {}\n",
        "\n",
        "        for (u, i), r in zip(X, y):\n",
        "            self.user_ratings.setdefault(u, []).append((i, r))\n",
        "            self.item_ratings.setdefault(i, []).append((u, r))\n",
        "\n",
        "        # ALS\n",
        "        for it in range(self.n_iter):\n",
        "\n",
        "            # ---- Update user factors ----\n",
        "            for u in range(self.n_users):\n",
        "\n",
        "                if u not in self.user_ratings:\n",
        "                    continue\n",
        "\n",
        "                items = [i for (i, _) in self.user_ratings[u]]\n",
        "                ratings = np.array([r for (_, r) in self.user_ratings[u]])\n",
        "\n",
        "                Q_sub = self.Q[items]\n",
        "\n",
        "                # Huber regression\n",
        "                huber = HuberRegressor(\n",
        "                    epsilon=self.delta,\n",
        "                    alpha=self.lam,\n",
        "                    fit_intercept=False,\n",
        "                    max_iter=200,\n",
        "                    warm_start=True\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    huber.fit(Q_sub, ratings)\n",
        "                    self.P[u] = huber.coef_\n",
        "                except:\n",
        "                    ridge = Ridge(alpha=self.lam, fit_intercept=False)\n",
        "                    ridge.fit(Q_sub, ratings)\n",
        "                    self.P[u] = ridge.coef_\n",
        "\n",
        "            # ---- Update item factors ----\n",
        "            for i in range(self.n_items):\n",
        "\n",
        "                if i not in self.item_ratings:\n",
        "                    continue\n",
        "\n",
        "                users = [u for (u, _) in self.item_ratings[i]]\n",
        "                ratings = np.array([r for (_, r) in self.item_ratings[i]])\n",
        "\n",
        "                P_sub = self.P[users]\n",
        "\n",
        "                huber = HuberRegressor(\n",
        "                    epsilon=self.delta,\n",
        "                    alpha=self.lam,\n",
        "                    fit_intercept=False,\n",
        "                    max_iter=200,\n",
        "                    warm_start=True\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    huber.fit(P_sub, ratings)\n",
        "                    self.Q[i] = huber.coef_\n",
        "                except:\n",
        "                    ridge = Ridge(alpha=self.lam, fit_intercept=False)\n",
        "                    ridge.fit(P_sub, ratings)\n",
        "                    self.Q[i] = ridge.coef_\n",
        "\n",
        "            print(f\"Iteration {it+1}/{self.n_iter} done.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for u, i in X:\n",
        "            preds.append(np.dot(self.P[u], self.Q[i]))\n",
        "        return np.array(preds)\n"
      ],
      "metadata": {
        "id": "86604hbxxc0B"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Netflix dataset\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/train.csv\")\n",
        "test  = pd.read_csv(\"https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/netflix/test.csv\")\n",
        "\n",
        "X_train = train[['user_id','movie_id']].values\n",
        "y_train = train['rating'].values\n",
        "\n",
        "X_test = test[['user_id','movie_id']].values\n",
        "y_test = test['rating'].values\n",
        "\n",
        "n_users = train.user_id.max()+1\n",
        "n_items = train.movie_id.max()+1\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# (1) λ=0.1, K=3, δ=1.35\n",
        "# -----------------------------------------------------\n",
        "model1 = Huber_SVD(n_users, n_items, K=3, lam=0.1, delta=1.35, n_iter=3)\n",
        "model1.fit(X_train, y_train)\n",
        "pred1 = model1.predict(X_test)\n",
        "print(\"RMSE (λ=0.1, K=3, δ=1.35):\", rmse(y_test, pred1))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# (2) λ=0.3, K=5, δ=1.5\n",
        "# -----------------------------------------------------\n",
        "model2 = Huber_SVD(n_users, n_items, K=5, lam=0.3, delta=1.5, n_iter=3)\n",
        "model2.fit(X_train, y_train)\n",
        "pred2 = model2.predict(X_test)\n",
        "print(\"RMSE (λ=0.3, K=5, δ=1.5):\", rmse(y_test, pred2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nony1SF_xfDg",
        "outputId": "b84e4867-f7ae-4460-d84c-53f0f9a701e1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/3 done.\n",
            "Iteration 2/3 done.\n",
            "Iteration 3/3 done.\n",
            "RMSE (λ=0.1, K=3, δ=1.35): 1.5267917045891513\n",
            "Iteration 1/3 done.\n",
            "Iteration 2/3 done.\n",
            "Iteration 3/3 done.\n",
            "RMSE (λ=0.3, K=5, δ=1.5): 1.2830423596328324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 4: Neural Recommender Systems"
      ],
      "metadata": {
        "id": "t_u9xRQblRTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/refs/heads/main/dataset/udemy/udemy_clean.csv')"
      ],
      "metadata": {
        "id": "vLlPqrj6mrQR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Preprocessing Side Info (Solution for Q4.1)\n",
        "Key Concept: Neural nets can take more than just ID. We need to scale dense features (age, time) and encode categorical ones (genre, gender)."
      ],
      "metadata": {
        "id": "VMBQNpm5lTkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical and dense features\n",
        "cat_features = ['Instructor', 'Level']\n",
        "dense_features = ['User_vote', 'Total_hours', 'Lecture']\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "label_encoders = {}\n",
        "\n",
        "for col in cat_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[dense_features] = scaler.fit_transform(df[dense_features])\n"
      ],
      "metadata": {
        "id": "fy87yzkblTAi"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Neural Architecture (PlainRS) (Solution for Q4.2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3LBEgRKlYV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "class PlainRS(Model):\n",
        "    def __init__(self, n_instructor, n_level, dense_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.instructor_emb = layers.Embedding(\n",
        "            input_dim=n_instructor,\n",
        "            output_dim=50,\n",
        "            embeddings_initializer='uniform'\n",
        "        )\n",
        "\n",
        "        self.level_emb = layers.Embedding(\n",
        "            input_dim=n_level,\n",
        "            output_dim=30,\n",
        "            embeddings_initializer='uniform'\n",
        "        )\n",
        "\n",
        "        # Dense features: (User_vote, Total_hours, Lecture)\n",
        "        self.dense_input = layers.Dense(dense_dim)\n",
        "\n",
        "        # Combined layers\n",
        "        self.concat = layers.Concatenate()\n",
        "        self.hidden = layers.Dense(64, activation='relu')\n",
        "        self.out_layer = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        instructor_id, level_id, dense_feats = inputs\n",
        "\n",
        "        ins_vec = self.instructor_emb(instructor_id)\n",
        "        lvl_vec = self.level_emb(level_id)\n",
        "\n",
        "        x = self.concat([ins_vec, lvl_vec, dense_feats])\n",
        "        x = self.hidden(x)\n",
        "        out = self.out_layer(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "o04Rca4AlZkK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/main/dataset/udemy/udemy_clean.csv')\n",
        "\n",
        "# Categorical encoders\n",
        "ins_enc = LabelEncoder()\n",
        "lvl_enc = LabelEncoder()\n",
        "\n",
        "df['Instructor_enc'] = ins_enc.fit_transform(df['Instructor'])\n",
        "df['Level_enc'] = lvl_enc.fit_transform(df['Level'])\n",
        "\n",
        "# Dense features\n",
        "dense_feats = ['User_vote', 'Total_hours', 'Lecture']\n",
        "scaler = StandardScaler()\n",
        "df[dense_feats] = scaler.fit_transform(df[dense_feats])\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare TF tensors\n",
        "X_train = (\n",
        "    train_df['Instructor_enc'].values,\n",
        "    train_df['Level_enc'].values,\n",
        "    train_df[dense_feats].values\n",
        ")\n",
        "y_train = train_df['Rating'].values\n",
        "\n",
        "X_test = (\n",
        "    test_df['Instructor_enc'].values,\n",
        "    test_df['Level_enc'].values,\n",
        "    test_df[dense_feats].values\n",
        ")\n",
        "y_test = test_df['Rating'].values\n"
      ],
      "metadata": {
        "id": "mAycixCFzkoo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PlainRS(\n",
        "    n_instructor=df['Instructor_enc'].nunique(),\n",
        "    n_level=df['Level_enc'].nunique(),\n",
        "    dense_dim=len(dense_feats)\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.losses.MeanAbsoluteError(), tf.keras.metrics.RootMeanSquaredError()]\n",
        ")\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_root_mean_squared_error',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=200,\n",
        "    batch_size=128,\n",
        "    callbacks=[callback],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A4vaBuAzmxV",
        "outputId": "001071a2-312f-4588-ee6a-d7d35425a8fe"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'plain_rs', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 13.3214 - mean_absolute_error: 3.5229 - root_mean_squared_error: 3.6285 - val_loss: 0.7471 - val_mean_absolute_error: 0.6545 - val_root_mean_squared_error: 0.8644\n",
            "Epoch 2/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.6393 - mean_absolute_error: 0.6043 - root_mean_squared_error: 0.7938 - val_loss: 0.2964 - val_mean_absolute_error: 0.4344 - val_root_mean_squared_error: 0.5444\n",
            "Epoch 3/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.2475 - mean_absolute_error: 0.3793 - root_mean_squared_error: 0.4973 - val_loss: 0.2309 - val_mean_absolute_error: 0.3839 - val_root_mean_squared_error: 0.4806\n",
            "Epoch 4/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.1707 - mean_absolute_error: 0.3142 - root_mean_squared_error: 0.4131 - val_loss: 0.2035 - val_mean_absolute_error: 0.3530 - val_root_mean_squared_error: 0.4512\n",
            "Epoch 5/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1240 - mean_absolute_error: 0.2600 - root_mean_squared_error: 0.3520 - val_loss: 0.1988 - val_mean_absolute_error: 0.3431 - val_root_mean_squared_error: 0.4458\n",
            "Epoch 6/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1006 - mean_absolute_error: 0.2241 - root_mean_squared_error: 0.3172 - val_loss: 0.2043 - val_mean_absolute_error: 0.3454 - val_root_mean_squared_error: 0.4520\n",
            "Epoch 7/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0934 - mean_absolute_error: 0.2095 - root_mean_squared_error: 0.3056 - val_loss: 0.2058 - val_mean_absolute_error: 0.3437 - val_root_mean_squared_error: 0.4537\n",
            "Epoch 8/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0902 - mean_absolute_error: 0.2016 - root_mean_squared_error: 0.3003 - val_loss: 0.2065 - val_mean_absolute_error: 0.3423 - val_root_mean_squared_error: 0.4544\n",
            "Epoch 9/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0848 - mean_absolute_error: 0.1958 - root_mean_squared_error: 0.2911 - val_loss: 0.2066 - val_mean_absolute_error: 0.3431 - val_root_mean_squared_error: 0.4545\n",
            "Epoch 10/200\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0864 - mean_absolute_error: 0.1971 - root_mean_squared_error: 0.2938 - val_loss: 0.2122 - val_mean_absolute_error: 0.3475 - val_root_mean_squared_error: 0.4607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, mae, rmse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test MAE:  {mae:.4f}\")\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItCWDSMozoD7",
        "outputId": "928bd15f-1be0-4233-d217-5cbe08131445"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE:  0.3502\n",
            "Test RMSE: 0.4553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Neural Architecture (DizzyRS) (Solution for Q4.3)\n"
      ],
      "metadata": {
        "id": "WZJaDrx6zsd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DizzyRS implementation (TensorFlow / Keras)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------\n",
        "# 1) Utilities\n",
        "# -----------------------\n",
        "class RMSE(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"rmse\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.sse = self.add_weight(name=\"sse\", initializer=\"zeros\")\n",
        "        self.n = self.add_weight(name=\"n\", initializer=\"zeros\")\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        se = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "        self.sse.assign_add(se)\n",
        "        self.n.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "    def result(self):\n",
        "        return tf.sqrt(self.sse / (self.n + 1e-12))\n",
        "    def reset_states(self):\n",
        "        self.sse.assign(0.0); self.n.assign(0.0)\n",
        "\n",
        "def rmse_np(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "\n",
        "# -----------------------\n",
        "# 2) Load & preprocess data (Q4 dataset)\n",
        "# -----------------------\n",
        "url = 'https://raw.githubusercontent.com/statmlben/CUHK-STAT3009/refs/heads/main/dataset/udemy/udemy_clean.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# features\n",
        "cat_features = ['Instructor', 'Level']\n",
        "dense_features = ['User_vote', 'Total_hours', 'Lecture']\n",
        "\n",
        "# Encode categoricals\n",
        "label_encoders = {}\n",
        "for col in cat_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col + '_enc'] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Standardize dense features\n",
        "scaler = StandardScaler()\n",
        "df[dense_features] = scaler.fit_transform(df[dense_features])\n",
        "\n",
        "# target: try common names\n",
        "if 'rating' in df.columns:\n",
        "    y_col = 'rating'\n",
        "elif 'Rating' in df.columns:\n",
        "    y_col = 'Rating'\n",
        "elif 'price' in df.columns:\n",
        "    y_col = 'price'\n",
        "else:\n",
        "    raise ValueError(\"Cannot find rating column; please inspect dataset.\")\n",
        "\n",
        "# prepare inputs\n",
        "X_instructor = df['Instructor_enc'].values.astype('int32')\n",
        "X_level = df['Level_enc'].values.astype('int32')\n",
        "X_dense = df[dense_features].values.astype('float32')\n",
        "y = df[y_col].values.astype('float32')\n",
        "\n",
        "# train/val/test split\n",
        "idx = np.arange(len(df))\n",
        "train_idx, test_idx = train_test_split(idx, test_size=0.15, random_state=42)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=42)\n",
        "\n",
        "def make_ds(idxs, batch=256, shuffle=False):\n",
        "    d = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'instr': X_instructor[idxs],\n",
        "            'lvl'  : X_level[idxs],\n",
        "            'dense': X_dense[idxs]\n",
        "        },\n",
        "        y[idxs]\n",
        "    ))\n",
        "    if shuffle:\n",
        "        d = d.shuffle(10000, seed=42)\n",
        "    d = d.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "    return d\n",
        "\n",
        "batch_size = 256\n",
        "train_ds = make_ds(train_idx, batch=batch_size, shuffle=True)\n",
        "val_ds   = make_ds(val_idx, batch=batch_size, shuffle=False)\n",
        "test_ds  = make_ds(test_idx, batch=batch_size, shuffle=False)\n",
        "\n",
        "n_instructor = int(df['Instructor_enc'].nunique())\n",
        "n_level = int(df['Level_enc'].nunique())\n",
        "n_dense = len(dense_features)\n",
        "\n",
        "# -----------------------\n",
        "# 3) DizzyRS model\n",
        "# -----------------------\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "class DizzyRS(Model):\n",
        "    def __init__(self,\n",
        "                 n_instructor,\n",
        "                 n_level,\n",
        "                 n_dense,\n",
        "                 instr_emb_dim=50,\n",
        "                 level_emb_dim_left=30,\n",
        "                 level_emb_dim_right=30,\n",
        "                 dense_proj_dim=32,\n",
        "                 hidden_units=64,\n",
        "                 dropout=0.2):\n",
        "        super().__init__()\n",
        "        # left branch embeddings\n",
        "        self.instr_emb = layers.Embedding(input_dim=n_instructor, output_dim=instr_emb_dim, name='instr_emb')\n",
        "        self.level_emb_left = layers.Embedding(input_dim=n_level, output_dim=level_emb_dim_left, name='level_emb_left')\n",
        "        # dense projection\n",
        "        self.dense_proj = layers.Dense(dense_proj_dim, activation='relu', name='dense_proj')\n",
        "        # left branch head\n",
        "        self.left_concat = layers.Concatenate(name='left_concat')\n",
        "        self.left_hidden = layers.Dense(hidden_units, activation='relu', name='left_hidden')\n",
        "        self.left_out = layers.Dense(1, name='left_out')  # produces out1\n",
        "\n",
        "        # right branch (separate level embedding)\n",
        "        self.level_emb_right = layers.Embedding(input_dim=n_level, output_dim=level_emb_dim_right, name='level_emb_right')\n",
        "        self.right_proj = layers.Dense(16, activation='relu', name='right_proj')\n",
        "        self.right_out = layers.Dense(1, name='right_out')  # produces out2\n",
        "\n",
        "        # optional dropout\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        instr = inputs['instr']    # shape (batch,)\n",
        "        lvl   = inputs['lvl']      # shape (batch,)\n",
        "        dense = inputs['dense']    # shape (batch, n_dense)\n",
        "\n",
        "        # left branch\n",
        "        instr_v = self.instr_emb(instr)           # (batch, instr_emb_dim)\n",
        "        lvl_v_left = self.level_emb_left(lvl)     # (batch, level_emb_dim_left)\n",
        "        dense_proj = self.dense_proj(dense)       # (batch, dense_proj_dim)\n",
        "        left_feat = self.left_concat([instr_v, lvl_v_left, dense_proj])\n",
        "        left_feat = self.dropout(left_feat, training=training)\n",
        "        left_h = self.left_hidden(left_feat)\n",
        "        out1 = self.left_out(left_h)              # (batch, 1)\n",
        "\n",
        "        # right branch\n",
        "        lvl_v_right = self.level_emb_right(lvl)   # (batch, level_emb_dim_right)\n",
        "        right_h = self.right_proj(lvl_v_right)\n",
        "        out2 = self.right_out(right_h)            # (batch, 1)\n",
        "\n",
        "        # sum outputs and squeeze\n",
        "        out = out1 + out2\n",
        "        return tf.squeeze(out, axis=-1)\n",
        "\n",
        "# -----------------------\n",
        "# 4) Instantiate, compile, train\n",
        "# -----------------------\n",
        "model = DizzyRS(n_instructor=n_instructor,\n",
        "                n_level=n_level,\n",
        "                n_dense=n_dense,\n",
        "                instr_emb_dim=50,\n",
        "                level_emb_dim_left=30,\n",
        "                level_emb_dim_right=30,\n",
        "                dense_proj_dim=32,\n",
        "                hidden_units=64,\n",
        "                dropout=0.2)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='mse',\n",
        "    metrics=[RMSE(), tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
        ")\n",
        "\n",
        "# callbacks: early stopping on validation RMSE (metric name 'val_rmse')\n",
        "earlystop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_rmse',\n",
        "    patience=5,\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=100,\n",
        "    callbacks=[earlystop, reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5) Evaluate\n",
        "# -----------------------\n",
        "res = model.evaluate(test_ds, return_dict=True)\n",
        "print(\"Test results:\", res)\n",
        "\n",
        "# -----------------------\n",
        "# 6) Example predictions on a small batch\n",
        "# -----------------------\n",
        "for batch_x, batch_y in test_ds.take(1):\n",
        "    preds = model.predict(batch_x)\n",
        "    print(\"sample preds:\", preds[:8])\n",
        "    print(\"sample truths:\", batch_y.numpy()[:8])\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3UMnXQz_xP",
        "outputId": "85ceda63-dc48-4c21-af43-f50ed5dd55c4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "28/28 - 3s - 111ms/step - loss: 8.0214 - mae: 2.5962 - rmse: 2.8322 - val_loss: 1.1463 - val_mae: 0.8770 - val_rmse: 1.0707 - learning_rate: 1.0000e-03\n",
            "Epoch 2/100\n",
            "28/28 - 0s - 10ms/step - loss: 0.8149 - mae: 0.6512 - rmse: 0.9027 - val_loss: 0.4387 - val_mae: 0.5403 - val_rmse: 0.6624 - learning_rate: 1.0000e-03\n",
            "Epoch 3/100\n",
            "28/28 - 0s - 8ms/step - loss: 0.3950 - mae: 0.4981 - rmse: 0.6285 - val_loss: 0.3049 - val_mae: 0.4295 - val_rmse: 0.5522 - learning_rate: 1.0000e-03\n",
            "Epoch 4/100\n",
            "28/28 - 0s - 8ms/step - loss: 0.3001 - mae: 0.4258 - rmse: 0.5478 - val_loss: 0.2511 - val_mae: 0.3904 - val_rmse: 0.5011 - learning_rate: 1.0000e-03\n",
            "Epoch 5/100\n",
            "28/28 - 0s - 14ms/step - loss: 0.2413 - mae: 0.3803 - rmse: 0.4912 - val_loss: 0.2180 - val_mae: 0.3620 - val_rmse: 0.4669 - learning_rate: 1.0000e-03\n",
            "Epoch 6/100\n",
            "28/28 - 1s - 21ms/step - loss: 0.1888 - mae: 0.3329 - rmse: 0.4345 - val_loss: 0.2038 - val_mae: 0.3511 - val_rmse: 0.4514 - learning_rate: 1.0000e-03\n",
            "Epoch 7/100\n",
            "28/28 - 1s - 22ms/step - loss: 0.1527 - mae: 0.2973 - rmse: 0.3908 - val_loss: 0.2006 - val_mae: 0.3507 - val_rmse: 0.4479 - learning_rate: 1.0000e-03\n",
            "Epoch 8/100\n",
            "28/28 - 1s - 18ms/step - loss: 0.1417 - mae: 0.2845 - rmse: 0.3765 - val_loss: 0.2002 - val_mae: 0.3493 - val_rmse: 0.4475 - learning_rate: 1.0000e-03\n",
            "Epoch 9/100\n",
            "28/28 - 1s - 34ms/step - loss: 0.1308 - mae: 0.2727 - rmse: 0.3617 - val_loss: 0.1979 - val_mae: 0.3464 - val_rmse: 0.4448 - learning_rate: 1.0000e-03\n",
            "Epoch 10/100\n",
            "28/28 - 1s - 32ms/step - loss: 0.1236 - mae: 0.2621 - rmse: 0.3516 - val_loss: 0.1992 - val_mae: 0.3481 - val_rmse: 0.4463 - learning_rate: 1.0000e-03\n",
            "Epoch 11/100\n",
            "28/28 - 1s - 27ms/step - loss: 0.1233 - mae: 0.2616 - rmse: 0.3512 - val_loss: 0.1974 - val_mae: 0.3467 - val_rmse: 0.4443 - learning_rate: 1.0000e-03\n",
            "Epoch 12/100\n",
            "28/28 - 0s - 16ms/step - loss: 0.1200 - mae: 0.2590 - rmse: 0.3463 - val_loss: 0.1992 - val_mae: 0.3477 - val_rmse: 0.4463 - learning_rate: 1.0000e-03\n",
            "Epoch 13/100\n",
            "28/28 - 0s - 8ms/step - loss: 0.1149 - mae: 0.2526 - rmse: 0.3390 - val_loss: 0.1980 - val_mae: 0.3473 - val_rmse: 0.4449 - learning_rate: 1.0000e-03\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "28/28 - 0s - 8ms/step - loss: 0.1138 - mae: 0.2497 - rmse: 0.3374 - val_loss: 0.1986 - val_mae: 0.3474 - val_rmse: 0.4456 - learning_rate: 1.0000e-03\n",
            "Epoch 15/100\n",
            "28/28 - 0s - 9ms/step - loss: 0.1107 - mae: 0.2474 - rmse: 0.3327 - val_loss: 0.1997 - val_mae: 0.3485 - val_rmse: 0.4468 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "28/28 - 0s - 8ms/step - loss: 0.1086 - mae: 0.2435 - rmse: 0.3295 - val_loss: 0.1992 - val_mae: 0.3481 - val_rmse: 0.4463 - learning_rate: 5.0000e-04\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2240 - mae: 0.3592 - rmse: 0.4733 \n",
            "Test results: {'loss': 0.21957452595233917, 'mae': 0.358569473028183, 'rmse': 0.4685877859592438}\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step  \n",
            "sample preds: [4.23336   3.8633897 3.7680821 3.4956279 3.6488025 4.2717032 3.3714182\n",
            " 4.005106 ]\n",
            "sample truths: [4.2 3.8 3.7 3.6 4.3 4.5 3.3 3.9]\n"
          ]
        }
      ]
    }
  ]
}